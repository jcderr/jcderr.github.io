---
layout: post
status: publish
published: true
title: Using Docker in Production
author:
  display_name: jeremy.derr
  login: jeremy.derr
  email: jeremy@derr.me
  url: ''
author_login: jeremy.derr
author_email: jeremy@derr.me
wordpress_id: 196
wordpress_url: http://continuously.me/?p=196
date: '2014-11-05 10:31:39 -0600'
date_gmt: '2014-11-05 16:31:39 -0600'
categories:
- Uncategorized
tags: []
comments: []
---
<p>I aggressively switched <a href="http://djed.com">Djed Studios</a> to Docker earlier this year and couldn't be happier with the result. It has been prominently front-and-center in our culture of continuous improvement and "failing fast". Initially, I recreated the exact, pre-existing development environment into a monolithic Docker container. I then slowly evolved the workflow such that we now do everything "The Docker Way" - we have 1 service per container and build our service up atomically from there. The very same containers that we use for development are, ultimately, placed onto our production servers.</p></p>
<h1>Python Onbuild</h1></p>
<p>The official <a href="https://github.com/docker-library/python/blob/master/2.7/onbuild/Dockerfile">python onbuild</a> containers proved invaluable. For a few reasons, though, I have forked them for our own purposes. You can see the base of <a href="https://github.com/jcderr/python">this fork on github</a>. The only notable change is that it is now based on ubuntu instead of debian.</p></p>
<p>Our private python-onbuild containers use this change to install a handful of apt packages that are integral to our site. In keeping with the purpose of onbuild containers, I wanted this container to be everything needed to build or run our app except for the actual python environment and code.</p></p>
<p>This gives us a really simple Dockerfile:</p></p>
<pre><code>FROM jcderr/python:2.7.8-onbuild</p>
<p>RUN ln -s /usr/src/app /opt/app; mkdir /opt/logs<br />
RUN mkdir -p /usr/src/app/static &amp;&amp; python manage.py collectstatic --noinput --clear<br />
VOLUME [ "/usr/src/app", "/var/run" ]<br />
CMD [ "python", "manage.py", "runserver", "0.0.0.0:80"]<br />
</code></pre></p>
<p>The 'onbuild' part of this construct is that the base image uses the <code>ONBUILD</code> declaration to grab our <code>requirements.txt</code> file and install a full python environment, and then injects our code into <code>/usr/src/app</code>. By doing this is two stages, building the python environment can utilize Docker's build cache if <code>requirements.txt</code> hasn't changed and results in a much faster build.</p></p>
<h1>Fig</h1></p>
<p>Fig 1.0 was <a href="https://blog.docker.com/2014/10/fig-1-0-boot2docker-compatibility-and-more/">released</a> simultaneously with <a href="https://blog.docker.com/2014/10/docker-1-3-signed-images-process-injection-security-options-mac-shared-directories/">Docker 1.3</a>. This was a key point for us, as it allowed us to retire <a href="https://github.com/jcderr/soos">Soos</a>, my custom-designed shell glue for our developers' environments. I had reviewed Fig at the time and found it lacking, but the fine folks at Orchard (and at Docker, after the acquisition) have done an incredible job in rapid iteration on the concept and produced an outrageously amazing product. The plan is to integrate it fully into a future release of Docker itself and I wholeheartedly endorse this effort. Fig is that awesome.</p></p>
<p>A shadow of Soos still persists in our project, mostly as a wrapper for common tasks in Fig, and we have two figfiles, <code>fig.yml</code> and <code>fig-production.yml</code>. By default, all <code>bin/soos</code> calls use <code>fig.yml</code>, which stands up a db, redis, and a Django dev server on port 8000, with all relevant links.</p></p>
<p>If someone calls <code>bin/soos --prod [action]</code>, however, it will use the production figfile. For instance, <code>bin/soos --prod up</code> will stand up a copy of the entire production stack locally, including Nginx with SSL, a celery worker, and so on.</p></p>
<h1>Production</h1></p>
<p>We deploy entirely within AWS with CloudFormation, and all of our instances are <a href="http://coreos.com">CoreOS</a> Stable machines. Each instance has EC2 metadata to define what environment it belongs to (prod, stage, or test) as well as its role (django, celery, etc).</p></p>
<h2>Sharing Secrets</h2></p>
<p>Securing and sharing secrets is one of the hard problems in operations. We chose to store these secrets in an S3 bucket and allow access via IAM profiles. Each environment has a matching environment file that defines a handful of necessary values, such as <code>DATABASE_URL</code>, in shell syntax.</p></p>
<h2>systemd</h2></p>
<p>systemd isn't for everyone, but it's the init system in CoreOS, it gets the job done, and it solves my specific problems quite well, so I've embraced it wholeheartedly in our stack. We have global units that target all units that match specific metadata (eg. 'role=django', 'role=celery').</p></p>
<p>I abuse subshells pretty aggressively, though. One of our units is a fork of the CoreOS team's example ELB Presence Notifier, altered to use IAM profiles instead of requiring access keys in the environment.</p></p>
<pre><code>[Unit]<br />
Description=ELB Presence Notifier<br />
After=http.service<br />
BindsTo=http.service</p>
<p>[Service]<br />
User=core<br />
TimeoutStartSec=0<br />
ExecStartPre=-/usr/bin/docker stop elb-presence<br />
ExecStartPre=-/usr/bin/docker rm elb-presence<br />
ExecStartPre=/usr/bin/docker pull jcderr/s3fetch<br />
ExecStartPre=-/usr/bin/docker run --rm -v /home/core:/root jcderr/s3fetch python /opt/app/s3fetch.py ************* .docker.cfg /root/.dockercfg<br />
ExecStartPre=-/bin/sh -c '/usr/bin/docker run --rm -v /home/core:/root jcderr/s3fetch python /opt/app/s3fetch.py ************* $(/usr/bin/etcdctl get /djed-environment) /root/$(/usr/bin/etcdctl get /djed-environment)'<br />
ExecStartPre=/usr/bin/docker pull djedstudios/elb-presence<br />
ExecStart=/bin/sh -c '/usr/bin/docker run --name elb-presence --env-file=/home/core/$(/usr/bin/etcdctl get /djed-environment) djedstudios/elb-presence'<br />
ExecStop=/usr/bin/docker stop elb-presence</p>
<p>[X-Fleet]<br />
Global=true<br />
MachineMetadata="role=django"<br />
</code></pre></p>
<p>This service adds the instance to an AWS Elastic Load Balancer and, when the linked <code>http.service</code> process dies, remove it again. Process termination is low hanging fruit. There's no point in waiting for the AWS health checker to take its time removing the instance if we know the process isn't even running.</p></p>
<p>As you can see, I have a key I fetch out of <code>etcd</code> to make this work. <code>djed-environment</code> holds the full path to the environment file we fetched from S3. Other units also rely on <code>djed-version</code>, which will be the tag name for the docker container that should be run.</p></p>
<p>With similarly constructed systemd units, I stand up the service like this:</p></p>
<ul>
<li>Fetch the app container</li>
<li>Run a copy of the app to serve the website</li>
<li>Run another copy to serve the API</li>
<li>Both run <code>uwsgi</code> on a Unix socket</li>
<li>Fetch and run the nginx container</li>
<li>Nginx picks up the app and API sockets as upstreams</li>
<li>Fetch and run the presence notifier</li><br />
</ul></p>
<p>We're a Django shop. The only difference between the <code>app</code> container and the <code>api</code> container is that we pass in a different <code>DJANGO_SETTINGS_MODULE</code> to the <code>api</code> container.</p></p>
<h1>Deploying Updates</h1></p>
<p>To deploy a new version of the code, I simply update the etcd key <code>/djed-version</code>. We have a self-healing/self-updating service called Norm that watches this key and, upon a change, begins the update process.</p></p>
<h2>Norm</h2></p>
<p>Norm is a short bash script that has two modes of operation: maintenance and update.</p></p>
<p>In Maintenance Mode, Norm watches the <code>fleet</code> service to make sure that all systemd units in the stack are healthy. If they fail, it works through a regimen of stopping, unloading, and then relaunching said services.</p></p>
<p>In Update mode, one machine is chosen at random to be the 'responsible adult' and the others are instructed to pull the targeted version of the container (as defined by the <code>/djed-version</code> key). Once each machine concludes the download, they inform the responsible adult by updating a key that includes their machine ID. Once the entire fleet has the new container, the responsible adult stops the service, performs all maintenance required in the upgrade (eg. database migrations, static file generation), and then brings the service back up.</p></p>
<p>In the near future, we'll be moving to a database migration scheme that will allow this update to take place without downtime, but we're not quite there yet.</p></p>
